{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# Load career HF dataset \n",
    "data_career = load_dataset(\"csv\", data_files=\"Career Dataset from HF.csv\")\n",
    "\n",
    "# Drop rows with missing input_text or target_text\n",
    "dataset_career = data_career.filter(lambda x: x[\"question\"] is not None and x[\"answer\"] is not None)\n",
    "\n",
    "dataset_career = dataset_career.rename_columns({\n",
    "    \"question\": \"input_text\",\n",
    "    \"answer\": \"target_text\"\n",
    "})\n",
    "dataset_career = dataset_career.remove_columns(\"role\")\n",
    "# Split into train/val (80/20 split)\n",
    "dataset_career_split = dataset_career[\"train\"].train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load resume dataset \n",
    "data_resume = load_dataset(\"csv\", data_files=\"2_formatted_resume_dataset.csv\")\n",
    "\n",
    "# Drop rows with missing input_text or target_text\n",
    "dataset_resume = data_resume.filter(lambda x: x[\"input_text\"] is not None and x[\"target_text\"] is not None)\n",
    "\n",
    "# Split into train/val (80/20 split)\n",
    "dataset_resume_split = dataset_resume[\"train\"].train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_text', 'target_text'],\n",
      "        num_rows: 518\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_text', 'target_text'],\n",
      "        num_rows: 130\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset_resume_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_text', 'target_text'],\n",
      "        num_rows: 1814\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_text', 'target_text'],\n",
      "        num_rows: 454\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets, DatasetDict\n",
    "\n",
    "# Join the train split\n",
    "train = concatenate_datasets([\n",
    "    #dataset_jd_split[\"train\"],\n",
    "    dataset_career_split[\"train\"],\n",
    "    dataset_resume_split[\"train\"]\n",
    "])\n",
    "\n",
    "# Join the test split\n",
    "test = concatenate_datasets([\n",
    "    #dataset_jd_split[\"test\"],\n",
    "    dataset_career_split[\"test\"],\n",
    "    dataset_resume_split[\"test\"]\n",
    "])\n",
    "\n",
    "split_dataset = DatasetDict({\n",
    "    \"train\": train,\n",
    "    \"test\": test\n",
    "})\n",
    "\n",
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d990669e4cda45c2a618e8e858374834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1814 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e8d7abf1274344976067bb46640a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/454 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "split_dataset.save_to_disk(\"dataset/split_combined_dataset_withoutjd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ZnXLGReyGZZY"
   },
   "outputs": [],
   "source": [
    "# Pre-processing the dataset to include the chat template that all LLaMA 3.2 3B models require\n",
    "\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "from datasets import disable_progress_bar\n",
    "\n",
    "disable_progress_bar()\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "instruction = \"\"\"You are a top-rated NTU career advisor chatbot.\n",
    "Be polite, concise, and helpful in providing career guidance responses.\"\"\"\n",
    "\n",
    "def format_chat_template(row):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": row[\"instruction\"]},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"response\"]}\n",
    "    ]\n",
    "\n",
    "    # Use the built-in chat template from Llama 3.2 Instruct\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False  # returns plain text\n",
    "    )\n",
    "    return row\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "    return model_inputs\n",
    "    \n",
    "formatted_dataset = split_dataset.map(format_chat_template)\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=formatted_dataset[\"train\"].column_names  # remove original text columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_text', 'target_text', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1814\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_text', 'target_text', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 454\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnJLa7ogZySE"
   },
   "source": [
    "## **Finding Optimal Values - Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sr5aGgj7feA6",
    "outputId": "4a51a432-5638-4017-a54f-66f09941542c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA support (modify according to your system and CUDA version)\n",
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "\n",
    "# Install Ray and Ray Tune\n",
    "!pip install ray[tune]\n",
    "\n",
    "!pip install datasets\n",
    "\n",
    "!pip install evaluate\n",
    "\n",
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install -U ipywidgets\n",
    "\n",
    "# Install Ray and Ray Tune\n",
    "!python3.11 -m pip install ray[tune]\n",
    "\n",
    "!python3.11 -m pip install bert-score ipywidgets sacrebleu evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<3,>=2.3 in ./.conda/envs/py311/lib/python3.11/site-packages (from bitsandbytes) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.conda/envs/py311/lib/python3.11/site-packages (from bitsandbytes) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.conda/envs/py311/lib/python3.11/site-packages (from bitsandbytes) (25.0)\n",
      "Requirement already satisfied: filelock in ./.conda/envs/py311/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.conda/envs/py311/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.conda/envs/py311/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.conda/envs/py311/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.conda/envs/py311/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.conda/envs/py311/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.conda/envs/py311/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.conda/envs/py311/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.conda/envs/py311/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.conda/envs/py311/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.conda/envs/py311/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.conda/envs/py311/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.conda/envs/py311/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.conda/envs/py311/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.conda/envs/py311/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.conda/envs/py311/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./.conda/envs/py311/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.conda/envs/py311/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.conda/envs/py311/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.conda/envs/py311/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in ./.conda/envs/py311/lib/python3.11/site-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in ./.conda/envs/py311/lib/python3.11/site-packages (from triton==3.4.0->torch<3,>=2.3->bitsandbytes) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.conda/envs/py311/lib/python3.11/site-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/envs/py311/lib/python3.11/site-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m  \u001b[33m0:00:10\u001b[0mm0:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.48.1\n"
     ]
    }
   ],
   "source": [
    "!python3.11 -m pip install bitsandbytes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9gpvLAgefnga",
    "outputId": "7b3a763f-4b04-4922-b3de-40f7e0094d8e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available in PyTorch: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 19:55:57,383\tINFO worker.py:2013 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray is initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FYP/shar0097/.conda/envs/py311/lib/python3.11/site-packages/ray/_private/worker.py:2052: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ray\n",
    "from ray import tune\n",
    "from bert_score import score\n",
    "\n",
    "# Verify CUDA support in PyTorch\n",
    "print(f\"CUDA available in PyTorch: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Check if Ray is ready\n",
    "ray.init(ignore_reinit_error=True)\n",
    "print(\"Ray is initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving to disk\n",
    "tokenized_dataset[\"train\"].save_to_disk(\"tokenized_train_dataset_3\")\n",
    "tokenized_dataset[\"test\"].save_to_disk(\"tokenized_test_dataset_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from disk\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "tokenized_dataset = load_from_disk(\"dataset/tokenized_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fyptf2tjVMSx",
    "outputId": "050dac13-16e7-45ee-fac4-97b419e3917c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs: 72\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Number of CPU cores available\n",
    "num_cpus = os.cpu_count()\n",
    "print(\"Number of CPUs:\", num_cpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q-kS3MM1VPvm",
    "outputId": "69d12e0b-60b0-48b8-e0ec-27c3b79e5a4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 1\n",
      "Tesla V100-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(\"Number of GPUs:\", num_gpus)\n",
    "    for i in range(num_gpus):\n",
    "        print(torch.cuda.get_device_name(i))\n",
    "else:\n",
    "    print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load my model\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "HUGGING_FACE_TOKEN = \"-----\" # Put in your Hugging Face token here\n",
    "\n",
    "small_train = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(900))  # 900 samples\n",
    "small_test = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(100))     # 100 samples\n",
    "\n",
    "def train_function_lora(config):\n",
    "    import gc, os\n",
    "    from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "    import evaluate\n",
    "\n",
    "    bleu = evaluate.load(\"sacrebleu\")\n",
    "    def compute_metrics(eval_pred, tokenizer):\n",
    "        logits, labels = eval_pred\n",
    "    \n",
    "        # Move logits to CPU to save GPU memory\n",
    "        logits = torch.tensor(logits).cpu()\n",
    "        labels = torch.tensor(labels).cpu()\n",
    "    \n",
    "        # Take argmax to get predicted token IDs\n",
    "        preds = torch.argmax(logits, dim=-1).numpy()\n",
    "    \n",
    "        # Replace -100 in labels (ignored tokens) with pad_token_id\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "        # Decode predictions and labels\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "        # Compute BLEU\n",
    "        bleu_score = bleu.compute(\n",
    "            predictions=decoded_preds,\n",
    "            references=[[l] for l in decoded_labels]\n",
    "        )[\"score\"]\n",
    "    \n",
    "        return {\"bleu\": bleu_score}\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    r = config[\"r\"]\n",
    "    alpha = config[\"alpha\"]\n",
    "    learning_rate = config[\"learning_rate\"]\n",
    "\n",
    "    print(f\"\\nTraining with r={r}, alpha={alpha}...\")\n",
    "\n",
    "    # Load model fresh each time\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        token=HUGGING_FACE_TOKEN,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=HUGGING_FACE_TOKEN)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # LoRA config\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=alpha,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    lora_model = get_peft_model(model, lora_cfg)\n",
    "\n",
    "    # Define Hugging Face training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results/r{r}_a{alpha}\",\n",
    "        learning_rate=learning_rate,   # from Ray Tune\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=8,  \n",
    "        #num_train_epochs=config[\"epochs\"],\n",
    "        num_train_epochs=2,\n",
    "        logging_dir=f\"./logs/r{r}_a{alpha}\",\n",
    "        logging_steps=100,\n",
    "        report_to=\"none\", \n",
    "        fp16=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=lora_model,\n",
    "        args=training_args,\n",
    "        train_dataset=small_train,\n",
    "        eval_dataset=small_test,\n",
    "        data_collator=data_collator,\n",
    "        #compute_metrics=compute_metrics, # compute later\n",
    "        compute_metrics=None,\n",
    "    )\n",
    "\n",
    "    # Train and evaluate\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    perplexity = torch.exp(torch.tensor(metrics[\"eval_loss\"])).item()\n",
    "\n",
    "    tune.report({\n",
    "        \"eval_loss\": metrics[\"eval_loss\"],\n",
    "        \"perplexity\": perplexity,\n",
    "        \"bleu\": metrics.get(\"bleu\", 0.0),\n",
    "    })\n",
    "\n",
    "    del model, lora_model, trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "upm7WXYk0_pw"
   },
   "outputs": [],
   "source": [
    "# Define the search space\n",
    "search_space = {\n",
    "    \"r\": tune.choice([16, 32, 64, 128, 256]),\n",
    "    \"alpha\": tune.choice([32,64, 128, 256]),\n",
    "    \"learning_rate\": tune.loguniform(1e-5, 1e-3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uIToggYN3I2W",
    "outputId": "05350360-37b0-486d-ecc7-57e93d04c1c2",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-10-15 01:20:06</td></tr>\n",
       "<tr><td>Running for: </td><td>01:02:42.92        </td></tr>\n",
       "<tr><td>Memory:      </td><td>19.7/377.5 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=3<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -0.5831849277019501<br>Logical resource usage: 0/72 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:V100)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  alpha</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  r</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  eval_loss</th><th style=\"text-align: right;\">  perplexity</th><th style=\"text-align: right;\">  bleu</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_function_lora_44212_00000</td><td>TERMINATED</td><td>10.128.10.15:308209</td><td style=\"text-align: right;\">    256</td><td style=\"text-align: right;\">    1.34172e-05</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         488.569</td><td style=\"text-align: right;\">   1.12392 </td><td style=\"text-align: right;\">     3.07689</td><td style=\"text-align: right;\">     0</td></tr>\n",
       "<tr><td>train_function_lora_44212_00001</td><td>TERMINATED</td><td>10.128.10.15:310220</td><td style=\"text-align: right;\">     64</td><td style=\"text-align: right;\">    0.000375887</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         455.969</td><td style=\"text-align: right;\">   0.539837</td><td style=\"text-align: right;\">     1.71573</td><td style=\"text-align: right;\">     0</td></tr>\n",
       "<tr><td>train_function_lora_44212_00002</td><td>TERMINATED</td><td>10.128.10.15:311584</td><td style=\"text-align: right;\">    256</td><td style=\"text-align: right;\">    0.000333228</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         464.207</td><td style=\"text-align: right;\">   0.53888 </td><td style=\"text-align: right;\">     1.71409</td><td style=\"text-align: right;\">     0</td></tr>\n",
       "<tr><td>train_function_lora_44212_00003</td><td>TERMINATED</td><td>10.128.10.15:312971</td><td style=\"text-align: right;\">    256</td><td style=\"text-align: right;\">    0.000770014</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         450.975</td><td style=\"text-align: right;\">   0.557507</td><td style=\"text-align: right;\">     1.74631</td><td style=\"text-align: right;\">     0</td></tr>\n",
       "<tr><td>train_function_lora_44212_00004</td><td>TERMINATED</td><td>10.128.10.15:314078</td><td style=\"text-align: right;\">     32</td><td style=\"text-align: right;\">    6.18333e-05</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         455.698</td><td style=\"text-align: right;\">   0.978025</td><td style=\"text-align: right;\">     2.6592 </td><td style=\"text-align: right;\">     0</td></tr>\n",
       "<tr><td>train_function_lora_44212_00005</td><td>TERMINATED</td><td>10.128.10.15:315565</td><td style=\"text-align: right;\">    128</td><td style=\"text-align: right;\">    5.5161e-05 </td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         453.127</td><td style=\"text-align: right;\">   0.702374</td><td style=\"text-align: right;\">     2.01854</td><td style=\"text-align: right;\">     0</td></tr>\n",
       "<tr><td>train_function_lora_44212_00006</td><td>TERMINATED</td><td>10.128.10.15:317139</td><td style=\"text-align: right;\">    128</td><td style=\"text-align: right;\">    6.98285e-05</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         464.142</td><td style=\"text-align: right;\">   0.608863</td><td style=\"text-align: right;\">     1.83834</td><td style=\"text-align: right;\">     0</td></tr>\n",
       "<tr><td>train_function_lora_44212_00007</td><td>TERMINATED</td><td>10.128.10.15:318495</td><td style=\"text-align: right;\">    256</td><td style=\"text-align: right;\">    0.000193407</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         464.087</td><td style=\"text-align: right;\">   0.537805</td><td style=\"text-align: right;\">     1.71224</td><td style=\"text-align: right;\">     0</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 01:20:06,306\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/FYP/shar0097/ray_results/train_function_lora_2025-10-15_00-17-23' in 0.0452s.\n",
      "2025-10-15 01:20:06,312\tINFO tune.py:1041 -- Total run time: 3763.25 seconds (3762.88 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r': 128, 'alpha': 256, 'learning_rate': 0.00019340692258392847}\n"
     ]
    }
   ],
   "source": [
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.tuner import Tuner\n",
    "from ray.tune import RunConfig\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"eval_loss\",z\n",
    "    mode=\"min\",\n",
    "    max_t=10,  # max epochs\n",
    "    grace_period=1,\n",
    "    reduction_factor=2,\n",
    ")\n",
    "\n",
    "trainable_with_cpu_gpu = tune.with_resources(train_function_lora, {\"cpu\": 4, \"gpu\": 1})\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    trainable_with_cpu_gpu,\n",
    "    param_space=search_space,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        scheduler=scheduler,\n",
    "        num_samples=8,  # number of trials\n",
    "        max_concurrent_trials=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "best_result = results.get_best_result(metric=\"eval_loss\", mode=\"min\")\n",
    "print(best_result.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = results.get_dataframe()\n",
    "df.to_csv(\"lora_tuning_results_14Oct(more data_2).csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df1 = results.get_dataframe()\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newly added with bert_score\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "HUGGING_FACE_TOKEN = \"-------\" # Put in your Hugging Face Token here\n",
    "\n",
    "small_train = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(900))  \n",
    "small_test = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(10))     \n",
    "\n",
    "def train_function_lora(config):\n",
    "    from bert_score import score as bert_score\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    import numpy as np\n",
    "    import csv\n",
    "    import gc, os, torch\n",
    "    from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "    import evaluate\n",
    "    from bert_score import BERTScorer\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        bleu_scorer = evaluate.load(\"sacrebleu\")\n",
    "        bert_scorer = BERTScorer(model_type=\"bert-base-uncased\", device=device)\n",
    "\n",
    "        # Hugging Face gives predictions and labels\n",
    "        preds, labels = eval_pred\n",
    "        # Decode predictions\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        # Replace -100 in labels with pad_token_id before decoding\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "        # BLEU expects list of list of refs\n",
    "        references = [[l] for l in decoded_labels]\n",
    "        bleu_score = bleu_scorer.compute(predictions=decoded_preds, references=references)[\"score\"]\n",
    "    \n",
    "        # BERTScore\n",
    "        P, R, F1 = bert_scorer.score(decoded_preds, decoded_labels)\n",
    "    \n",
    "        return {\n",
    "            \"bleu\": bleu_score,\n",
    "            \"bert_precision\": P.mean().item(),\n",
    "            \"bert_recall\": R.mean().item(),\n",
    "            \"bert_f1\": F1.mean().item()\n",
    "        }\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    r = config[\"r\"]\n",
    "    alpha = config[\"alpha\"]\n",
    "    learning_rate = config[\"learning_rate\"]\n",
    "\n",
    "    print(f\"\\nTraining with r={r}, alpha={alpha}...\")\n",
    "\n",
    "    compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "    # bits and bytes config for qlora \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type='nf4',\n",
    "            bnb_4bit_compute_dtype=compute_dtype,\n",
    "            bnb_4bit_use_double_quant=False,\n",
    "        )\n",
    "\n",
    "    # Load model fresh each time\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        token=HUGGING_FACE_TOKEN,\n",
    "        quantization_config=bnb_config # qlora\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=HUGGING_FACE_TOKEN)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # LoRA config\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=alpha,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    lora_model = get_peft_model(model, lora_cfg)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results/r{r}_a{alpha}\",\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=8,  \n",
    "        num_train_epochs=2,\n",
    "        logging_dir=f\"./logs/r{r}_a{alpha}\",\n",
    "        logging_steps=100,\n",
    "        report_to=\"none\",\n",
    "        fp16=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=lora_model,\n",
    "        args=training_args,\n",
    "        train_dataset=small_train,\n",
    "        eval_dataset=small_test,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=None,\n",
    "    )\n",
    "\n",
    "    # Train and evaluate\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    perplexity = torch.exp(torch.tensor(metrics[\"eval_loss\"])).item()\n",
    "\n",
    "    # Report to Ray Tune - eval_loss  perplexity bleu  bert_precision  bert_recall   bert_f1\n",
    "    tune.report({\n",
    "        \"eval_loss\": metrics[\"eval_loss\"],\n",
    "        \"perplexity\": perplexity,\n",
    "        #\"bleu\": metrics.get(\"bleu\", 0.0),\n",
    "        #\"bert_precision\": metrics.get(\"bert_precision\", 0.0),\n",
    "        #\"bert_recall\": metrics.get(\"bert_recall\", 0.0),\n",
    "        #\"bert_f1\": metrics.get(\"bert_f1\", 0.0),\n",
    "    })\n",
    "\n",
    "    del model, lora_model, trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search space\n",
    "search_space = {\n",
    "    \"r\": tune.choice([16, 32, 64, 128, 256]),\n",
    "    \"alpha\": tune.choice([32, 64, 128, 256]),\n",
    "    \"learning_rate\": tune.loguniform(1e-5, 1e-3),S\n",
    "    # \"batch_size\": tune.choice([16, 32]),\n",
    "    #\"epochs\": tune.choice([2, 4]),\n",
    "    # \"optimizer\": tune.choice([\"adam\", \"sgd\"]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-10-15 02:29:42</td></tr>\n",
       "<tr><td>Running for: </td><td>01:09:35.89        </td></tr>\n",
       "<tr><td>Memory:      </td><td>17.7/377.5 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=2<br>Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -0.4560428708791733<br>Logical resource usage: 0/72 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:V100)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                     </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  alpha</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  r</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  eval_loss</th><th style=\"text-align: right;\">  perplexity</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_function_lora_075ea_00000</td><td>TERMINATED</td><td>10.128.10.15:319394</td><td style=\"text-align: right;\">     64</td><td style=\"text-align: right;\">    9.38677e-05</td><td style=\"text-align: right;\">256</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         533.591</td><td style=\"text-align: right;\">   0.523762</td><td style=\"text-align: right;\">     1.68837</td></tr>\n",
       "<tr><td>train_function_lora_075ea_00001</td><td>TERMINATED</td><td>10.128.10.15:320430</td><td style=\"text-align: right;\">    128</td><td style=\"text-align: right;\">    0.000560656</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         507.834</td><td style=\"text-align: right;\">   0.452511</td><td style=\"text-align: right;\">     1.57226</td></tr>\n",
       "<tr><td>train_function_lora_075ea_00002</td><td>TERMINATED</td><td>10.128.10.15:321754</td><td style=\"text-align: right;\">     64</td><td style=\"text-align: right;\">    4.80874e-05</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         513.855</td><td style=\"text-align: right;\">   0.741647</td><td style=\"text-align: right;\">     2.09939</td></tr>\n",
       "<tr><td>train_function_lora_075ea_00003</td><td>TERMINATED</td><td>10.128.10.15:324184</td><td style=\"text-align: right;\">    128</td><td style=\"text-align: right;\">    0.000347184</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         513.759</td><td style=\"text-align: right;\">   0.448758</td><td style=\"text-align: right;\">     1.56637</td></tr>\n",
       "<tr><td>train_function_lora_075ea_00004</td><td>TERMINATED</td><td>10.128.10.15:325190</td><td style=\"text-align: right;\">    256</td><td style=\"text-align: right;\">    0.000882962</td><td style=\"text-align: right;\">256</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         516.846</td><td style=\"text-align: right;\">   0.45713 </td><td style=\"text-align: right;\">     1.57953</td></tr>\n",
       "<tr><td>train_function_lora_075ea_00005</td><td>TERMINATED</td><td>10.128.10.15:326635</td><td style=\"text-align: right;\">    256</td><td style=\"text-align: right;\">    0.000513624</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         514.369</td><td style=\"text-align: right;\">   0.450956</td><td style=\"text-align: right;\">     1.56981</td></tr>\n",
       "<tr><td>train_function_lora_075ea_00006</td><td>TERMINATED</td><td>10.128.10.15:328341</td><td style=\"text-align: right;\">    128</td><td style=\"text-align: right;\">    0.000289956</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         513.843</td><td style=\"text-align: right;\">   0.460199</td><td style=\"text-align: right;\">     1.58439</td></tr>\n",
       "<tr><td>train_function_lora_075ea_00007</td><td>TERMINATED</td><td>10.128.10.15:329714</td><td style=\"text-align: right;\">    128</td><td style=\"text-align: right;\">    0.000647171</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         516.179</td><td style=\"text-align: right;\">   0.454956</td><td style=\"text-align: right;\">     1.5761 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 02:29:42,514\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/FYP/shar0097/ray_results/train_function_lora_2025-10-15_01-20-06' in 0.2825s.\n",
      "2025-10-15 02:29:42,519\tINFO tune.py:1041 -- Total run time: 4175.91 seconds (4175.61 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r': 16, 'alpha': 128, 'learning_rate': 0.0003471842340392062}\n"
     ]
    }
   ],
   "source": [
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.tuner import Tuner\n",
    "from ray.tune import RunConfig\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"eval_loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=10,  # max epochs\n",
    "    grace_period=1,\n",
    "    reduction_factor=2,\n",
    ")\n",
    "\n",
    "trainable_with_cpu_gpu = tune.with_resources(train_function_lora, {\"cpu\": 4, \"gpu\": 1})\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    trainable_with_cpu_gpu,\n",
    "    param_space=search_space,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        scheduler=scheduler,\n",
    "        num_samples=8,  # number of trials\n",
    "        max_concurrent_trials=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "best_result = results.get_best_result(metric=\"eval_loss\", mode=\"min\")\n",
    "print(best_result.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = results.get_dataframe()\n",
    "df.to_csv(\"qlora_tuning_results_14Oct(more data_2).csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the LoRA and QLoRA models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93fd166893b843eeaadb2a77bcd6a3c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "HUGGING_FACE_TOKEN = \"------\" #Put in your Hugging Face Token here\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=HUGGING_FACE_TOKEN,\n",
    "    device_map='auto',\n",
    "    dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True      # efficient loading\n",
    ")\n",
    "\n",
    "# Set token\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=HUGGING_FACE_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 : Freezing the Model’s Parameters\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = False # model's parameters are frozen during training, gradients for these parameters are not computed during backpropogation\n",
    "  if param.ndim == 1: # checks if the parameter is 1-d and then converts the data to float32 type, to have mixed-precision training\n",
    "    param.data = param.data.to(torch.float32)\n",
    "#  to have the main computations done in float16 (to speed up training and reduce memory consumption), but certain parameters—like biases—are kept in float32 to avoid numerical instability.\n",
    "\n",
    "  model.gradient_checkpointing_enable() # memory-saving technique,  instead of storing all intermediate activations needed for backpropagation, the model recomputes some activations during the backward pass\n",
    "  model.enable_input_require_grads() # gradients are calculated for the model’s inputs, which can be useful when you need to compute gradients with respect to the input data\n",
    "\n",
    "# forward method overrides the default behavior and ensures that the output of the model's forward pass is cast to torch.float32\n",
    "  class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x) :\n",
    "      return super().forward(x).to(torch.float32)  # Cast to float32 after\n",
    "\n",
    "# typically refers to the final output layer of the language model\n",
    "# output from the language model head is always in float32 precision\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 : Checking Trainable Parameters\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "  printing the number of trainable paramters in the model\n",
    "  \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.0.self_attn.q_proj\n",
      "  - \n",
      "model.layers.0.self_attn.k_proj\n",
      "  - \n",
      "model.layers.0.self_attn.v_proj\n",
      "  - \n",
      "model.layers.0.self_attn.o_proj\n",
      "  - \n",
      "model.layers.0.post_attention_layernorm\n",
      "  - \n",
      "model.layers.1.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.1.self_attn.q_proj\n",
      "  - \n",
      "model.layers.1.self_attn.k_proj\n",
      "  - \n",
      "model.layers.1.self_attn.v_proj\n",
      "  - \n",
      "model.layers.1.self_attn.o_proj\n",
      "  - \n",
      "model.layers.1.post_attention_layernorm\n",
      "  - \n",
      "model.layers.2.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.2.self_attn.q_proj\n",
      "  - \n",
      "model.layers.2.self_attn.k_proj\n",
      "  - \n",
      "model.layers.2.self_attn.v_proj\n",
      "  - \n",
      "model.layers.2.self_attn.o_proj\n",
      "  - \n",
      "model.layers.2.post_attention_layernorm\n",
      "  - \n",
      "model.layers.3.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.3.self_attn.q_proj\n",
      "  - \n",
      "model.layers.3.self_attn.k_proj\n",
      "  - \n",
      "model.layers.3.self_attn.v_proj\n",
      "  - \n",
      "model.layers.3.self_attn.o_proj\n",
      "  - \n",
      "model.layers.3.post_attention_layernorm\n",
      "  - \n",
      "model.layers.4.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.4.self_attn.q_proj\n",
      "  - \n",
      "model.layers.4.self_attn.k_proj\n",
      "  - \n",
      "model.layers.4.self_attn.v_proj\n",
      "  - \n",
      "model.layers.4.self_attn.o_proj\n",
      "  - \n",
      "model.layers.4.post_attention_layernorm\n",
      "  - \n",
      "model.layers.5.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.5.self_attn.q_proj\n",
      "  - \n",
      "model.layers.5.self_attn.k_proj\n",
      "  - \n",
      "model.layers.5.self_attn.v_proj\n",
      "  - \n",
      "model.layers.5.self_attn.o_proj\n",
      "  - \n",
      "model.layers.5.post_attention_layernorm\n",
      "  - \n",
      "model.layers.6.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.6.self_attn.q_proj\n",
      "  - \n",
      "model.layers.6.self_attn.k_proj\n",
      "  - \n",
      "model.layers.6.self_attn.v_proj\n",
      "  - \n",
      "model.layers.6.self_attn.o_proj\n",
      "  - \n",
      "model.layers.6.post_attention_layernorm\n",
      "  - \n",
      "model.layers.7.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.7.self_attn.q_proj\n",
      "  - \n",
      "model.layers.7.self_attn.k_proj\n",
      "  - \n",
      "model.layers.7.self_attn.v_proj\n",
      "  - \n",
      "model.layers.7.self_attn.o_proj\n",
      "  - \n",
      "model.layers.7.post_attention_layernorm\n",
      "  - \n",
      "model.layers.8.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.8.self_attn.q_proj\n",
      "  - \n",
      "model.layers.8.self_attn.k_proj\n",
      "  - \n",
      "model.layers.8.self_attn.v_proj\n",
      "  - \n",
      "model.layers.8.self_attn.o_proj\n",
      "  - \n",
      "model.layers.8.post_attention_layernorm\n",
      "  - \n",
      "model.layers.9.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.9.self_attn.q_proj\n",
      "  - \n",
      "model.layers.9.self_attn.k_proj\n",
      "  - \n",
      "model.layers.9.self_attn.v_proj\n",
      "  - \n",
      "model.layers.9.self_attn.o_proj\n",
      "  - \n",
      "model.layers.9.post_attention_layernorm\n",
      "  - \n",
      "model.layers.10.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.10.self_attn.q_proj\n",
      "  - \n",
      "model.layers.10.self_attn.k_proj\n",
      "  - \n",
      "model.layers.10.self_attn.v_proj\n",
      "  - \n",
      "model.layers.10.self_attn.o_proj\n",
      "  - \n",
      "model.layers.10.post_attention_layernorm\n",
      "  - \n",
      "model.layers.11.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.11.self_attn.q_proj\n",
      "  - \n",
      "model.layers.11.self_attn.k_proj\n",
      "  - \n",
      "model.layers.11.self_attn.v_proj\n",
      "  - \n",
      "model.layers.11.self_attn.o_proj\n",
      "  - \n",
      "model.layers.11.post_attention_layernorm\n",
      "  - \n",
      "model.layers.12.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.12.self_attn.q_proj\n",
      "  - \n",
      "model.layers.12.self_attn.k_proj\n",
      "  - \n",
      "model.layers.12.self_attn.v_proj\n",
      "  - \n",
      "model.layers.12.self_attn.o_proj\n",
      "  - \n",
      "model.layers.12.post_attention_layernorm\n",
      "  - \n",
      "model.layers.13.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.13.self_attn.q_proj\n",
      "  - \n",
      "model.layers.13.self_attn.k_proj\n",
      "  - \n",
      "model.layers.13.self_attn.v_proj\n",
      "  - \n",
      "model.layers.13.self_attn.o_proj\n",
      "  - \n",
      "model.layers.13.post_attention_layernorm\n",
      "  - \n",
      "model.layers.14.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.14.self_attn.q_proj\n",
      "  - \n",
      "model.layers.14.self_attn.k_proj\n",
      "  - \n",
      "model.layers.14.self_attn.v_proj\n",
      "  - \n",
      "model.layers.14.self_attn.o_proj\n",
      "  - \n",
      "model.layers.14.post_attention_layernorm\n",
      "  - \n",
      "model.layers.15.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.15.self_attn.q_proj\n",
      "  - \n",
      "model.layers.15.self_attn.k_proj\n",
      "  - \n",
      "model.layers.15.self_attn.v_proj\n",
      "  - \n",
      "model.layers.15.self_attn.o_proj\n",
      "  - \n",
      "model.layers.15.post_attention_layernorm\n",
      "  - \n",
      "model.layers.16.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.16.self_attn.q_proj\n",
      "  - \n",
      "model.layers.16.self_attn.k_proj\n",
      "  - \n",
      "model.layers.16.self_attn.v_proj\n",
      "  - \n",
      "model.layers.16.self_attn.o_proj\n",
      "  - \n",
      "model.layers.16.post_attention_layernorm\n",
      "  - \n",
      "model.layers.17.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.17.self_attn.q_proj\n",
      "  - \n",
      "model.layers.17.self_attn.k_proj\n",
      "  - \n",
      "model.layers.17.self_attn.v_proj\n",
      "  - \n",
      "model.layers.17.self_attn.o_proj\n",
      "  - \n",
      "model.layers.17.post_attention_layernorm\n",
      "  - \n",
      "model.layers.18.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.18.self_attn.q_proj\n",
      "  - \n",
      "model.layers.18.self_attn.k_proj\n",
      "  - \n",
      "model.layers.18.self_attn.v_proj\n",
      "  - \n",
      "model.layers.18.self_attn.o_proj\n",
      "  - \n",
      "model.layers.18.post_attention_layernorm\n",
      "  - \n",
      "model.layers.19.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.19.self_attn.q_proj\n",
      "  - \n",
      "model.layers.19.self_attn.k_proj\n",
      "  - \n",
      "model.layers.19.self_attn.v_proj\n",
      "  - \n",
      "model.layers.19.self_attn.o_proj\n",
      "  - \n",
      "model.layers.19.post_attention_layernorm\n",
      "  - \n",
      "model.layers.20.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.20.self_attn.q_proj\n",
      "  - \n",
      "model.layers.20.self_attn.k_proj\n",
      "  - \n",
      "model.layers.20.self_attn.v_proj\n",
      "  - \n",
      "model.layers.20.self_attn.o_proj\n",
      "  - \n",
      "model.layers.20.post_attention_layernorm\n",
      "  - \n",
      "model.layers.21.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.21.self_attn.q_proj\n",
      "  - \n",
      "model.layers.21.self_attn.k_proj\n",
      "  - \n",
      "model.layers.21.self_attn.v_proj\n",
      "  - \n",
      "model.layers.21.self_attn.o_proj\n",
      "  - \n",
      "model.layers.21.post_attention_layernorm\n",
      "  - \n",
      "model.layers.22.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.22.self_attn.q_proj\n",
      "  - \n",
      "model.layers.22.self_attn.k_proj\n",
      "  - \n",
      "model.layers.22.self_attn.v_proj\n",
      "  - \n",
      "model.layers.22.self_attn.o_proj\n",
      "  - \n",
      "model.layers.22.post_attention_layernorm\n",
      "  - \n",
      "model.layers.23.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.23.self_attn.q_proj\n",
      "  - \n",
      "model.layers.23.self_attn.k_proj\n",
      "  - \n",
      "model.layers.23.self_attn.v_proj\n",
      "  - \n",
      "model.layers.23.self_attn.o_proj\n",
      "  - \n",
      "model.layers.23.post_attention_layernorm\n",
      "  - \n",
      "model.layers.24.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.24.self_attn.q_proj\n",
      "  - \n",
      "model.layers.24.self_attn.k_proj\n",
      "  - \n",
      "model.layers.24.self_attn.v_proj\n",
      "  - \n",
      "model.layers.24.self_attn.o_proj\n",
      "  - \n",
      "model.layers.24.post_attention_layernorm\n",
      "  - \n",
      "model.layers.25.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.25.self_attn.q_proj\n",
      "  - \n",
      "model.layers.25.self_attn.k_proj\n",
      "  - \n",
      "model.layers.25.self_attn.v_proj\n",
      "  - \n",
      "model.layers.25.self_attn.o_proj\n",
      "  - \n",
      "model.layers.25.post_attention_layernorm\n",
      "  - \n",
      "model.layers.26.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.26.self_attn.q_proj\n",
      "  - \n",
      "model.layers.26.self_attn.k_proj\n",
      "  - \n",
      "model.layers.26.self_attn.v_proj\n",
      "  - \n",
      "model.layers.26.self_attn.o_proj\n",
      "  - \n",
      "model.layers.26.post_attention_layernorm\n",
      "  - \n",
      "model.layers.27.self_attn\n",
      "  - \n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - o_proj\n",
      "model.layers.27.self_attn.q_proj\n",
      "  - \n",
      "model.layers.27.self_attn.k_proj\n",
      "  - \n",
      "model.layers.27.self_attn.v_proj\n",
      "  - \n",
      "model.layers.27.self_attn.o_proj\n",
      "  - \n",
      "model.layers.27.post_attention_layernorm\n",
      "  - \n"
     ]
    }
   ],
   "source": [
    "# Step 5 : Setting up LoRA Configuration\n",
    "for name, module in model.named_modules():\n",
    "    if 'attn' in name or 'attention' in name:  # Common attention module names\n",
    "        print(name)\n",
    "        for sub_name, sub_module in module.named_modules():  # Check sub-modules within attention\n",
    "            print(f\"  - {sub_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r = 256, # optimized\n",
    "    lora_alpha = 128, # # optimized\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], # This specifies which layers (or submodules) in the model will be adapted using LoRA\n",
    "    lora_dropout = 0.05, # regularization technique that helps prevent overfitting\n",
    "    bias = \"none\", #how to handle the bias terms in the model during fine-tuning, so in this case, no bias terms are updated or fine-tuned during the LoRA process\n",
    "    task_type = \"CAUSAL_LM\" #  specifies the type of task for which the model is being fine-tuned\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r = 256, # optimized\n",
    "    lora_alpha = 128, # # optimized\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], # Submodules \n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\", \n",
    "    task_type = \"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 146800640 || all params: 3359550464 || trainable%: 4.369651284392792\n"
     ]
    }
   ],
   "source": [
    "# Step 6 : Injecting LoRA into the Model and comparing trainable parameters\n",
    "lora_model = get_peft_model(model, config)\n",
    "print_trainable_parameters(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [114/114 06:50, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.895900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=114, training_loss=0.8247662761755157, metrics={'train_runtime': 415.2161, 'train_samples_per_second': 8.738, 'train_steps_per_second': 0.275, 'total_flos': 1.6525836687507456e+16, 'train_loss': 0.8247662761755157, 'epoch': 2.0})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "trainer = transformers.Trainer(\n",
    "    model = lora_model,\n",
    "\n",
    "    train_dataset = tokenized_dataset['train'],\n",
    "    args = transformers.TrainingArguments(\n",
    "        per_device_eval_batch_size = 1,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 10,\n",
    "        learning_rate = 3.44e-04, # optimized\n",
    "        fp16 = True,\n",
    "        num_train_epochs=2,\n",
    "        logging_steps = 100,\n",
    "        output_dir = 'outputs',\n",
    "        report_to = \"none\"\n",
    "    ),\n",
    "    data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm = False)\n",
    ")\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7a4fc3737a4556b9e5800be6f359e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FYP/shar0097/.conda/envs/py311/lib/python3.11/site-packages/transformers/utils/hub.py:917: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19098d8b7f0144da8c8ec12d1b9fcdbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcef90f437ff49a78a81a42963160f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/sharshar20/career-advisory-lora-llama3.2-3b-instruct-v7/commit/5e6f19c2f13a749783020c40d653fccb45d93b9c', commit_message='Lora Training method for Instruct Model(edited dataset), r=256, alpha=128, lr=3.44e-04', commit_description='', oid='5e6f19c2f13a749783020c40d653fccb45d93b9c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/sharshar20/career-advisory-lora-llama3.2-3b-instruct-v7', endpoint='https://huggingface.co', repo_type='model', repo_id='sharshar20/career-advisory-lora-llama3.2-3b-instruct-v7'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sharshar20/career-advisory-lora-llama3.2-3b-v2\n",
    "\n",
    "lora_model.push_to_hub(\"sharshar20/career-advisory-lora-llama3.2-3b-instruct-v7\",\n",
    "                      use_auth_token=True,\n",
    "                      commit_message = \"Lora Training method for Instruct Model(edited dataset), r=256, alpha=128, lr=3.44e-04\",\n",
    "                      private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecea7ed425554925a698fc65cc72151b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 10 : Inferencing with trained LoRA adapter - merging both base model and lora adapters\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "peft_model_id = \"sharshar20/career-advisory-lora-llama3.2-3b-instruct-v7\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model_lora = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\n",
    "                                            return_dict = True,\n",
    "                                            device_map = 'auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "modelLoRA = PeftModel.from_pretrained(model_lora,peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from bert_score import BERTScorer\n",
    "\n",
    "def evaluate_bertscore(model, tokenizer, dataset, max_new_tokens=128, batch_size=8, print_every=10):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    \n",
    "    scorer = BERTScorer(model_type=\"bert-base-uncased\", device=device)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_refs = []\n",
    "\n",
    "    num_samples = len(dataset)\n",
    "    \n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch = dataset[i:i+batch_size]\n",
    "\n",
    "        input_ids = torch.tensor(batch[\"input_ids\"]).to(device)\n",
    "        attention_mask = torch.tensor(batch[\"attention_mask\"]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=max_new_tokens\n",
    "            )\n",
    "\n",
    "        # Decode predictions\n",
    "        decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "        all_preds.extend(decoded_preds)\n",
    "        all_refs.extend(decoded_labels)\n",
    "\n",
    "        # Print intermediate BERTScore every `print_every` batches\n",
    "        if (i // batch_size + 1) % print_every == 0 or (i + batch_size) >= num_samples:\n",
    "            P, R, F1 = scorer.score(all_preds, all_refs)\n",
    "            print(f\"After batch {i//batch_size + 1}: BERT F1 = {F1.mean().item():.4f}\")\n",
    "\n",
    "    # Compute BERTScore\n",
    "    P, R, F1 = scorer.score(all_preds, all_refs)\n",
    "    \n",
    "    return {\n",
    "        \"bert_precision\": P.mean().item(),\n",
    "        \"bert_recall\": R.mean().item(),\n",
    "        \"bert_f1\": F1.mean().item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 10: BERT F1 = 0.9307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 20: BERT F1 = 0.9353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 30: BERT F1 = 0.9365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 40: BERT F1 = 0.9361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 50: BERT F1 = 0.9254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 57: BERT F1 = 0.9207\n",
      "{'bert_precision': 0.8791199922561646, 'bert_recall': 0.9683712720870972, 'bert_f1': 0.9207432866096497}\n"
     ]
    }
   ],
   "source": [
    "# BERTScore evaluation for modelLoRA\n",
    "metrics = evaluate_bertscore(modelLoRA, tokenizer, tokenized_dataset[\"test\"])\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 10: BERT F1 = 0.9289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 20: BERT F1 = 0.9306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 30: BERT F1 = 0.9329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 40: BERT F1 = 0.9317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 50: BERT F1 = 0.9227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 57: BERT F1 = 0.9187\n",
      "{'bert_precision': 0.8749769330024719, 'bert_recall': 0.9689300656318665, 'bert_f1': 0.9187436699867249}\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_bertscore(model_lora, tokenizer, tokenized_dataset[\"test\"])\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Response:  Considering your engineering background and python skill, Careers like Data Scientist, Machine Learning Engineer, AI Researcher etc., can be considered.\n",
      "\n",
      "\n",
      "Note: The above responses are designed to provide brief and concise answers that cater to different personality types. They aim to simulate real-life conversations between a career counselor and a client. \n",
      "\n",
      "The goal is to encourage users to explore various fields and industries by highlighting their strengths and transferable skills. By doing so, we hope to\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and move the model to the appropriate device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "modelLoRA.to(device)  # Move the model to the chosen device\n",
    "\n",
    "# Define the input prompt/question\n",
    "cot_examples = \"\"\"\n",
    "You are a helpful and smart career advisor. Always answer concisely in 1-2 sentences\n",
    "by providing career suggestions based on the user's skills, education and interests. \n",
    "Avoid listing too many tools, libraries, career options and responsibilities.\n",
    "\n",
    "Q: I enjoy working with numbers, have a degree in economics, and experience in Excel and SQL. What careers suit me? \n",
    "A: Since you have an economics background and know tools like Excel and SQL, I’d suggest careers such as data analyst, financial analyst, or business intelligence.\n",
    "\n",
    "Q: I am good at creative writing, storytelling, and content creation. I also know basic graphic design and social media marketing. I have a degree in Communications. What career paths should I consider?\n",
    "A: With your mix of writing, design, and marketing skills, I’d recommend careers like content creator, social media manager, or digital marketing specialist.\n",
    "\n",
    "Q: I have experience in project management, leadership, and team coordination. I also understand budgeting and risk management. I hold an MBA. Which careers suit me?\n",
    "A: Given your leadership and management background, plus an MBA, careers such as project manager or management consultant would suit you.\n",
    "\n",
    "Q: I am passionate about biology and healthcare. I have laboratory experience, strong analytical skills, and a degree in Biochemistry. What careers should I consider?\n",
    "A: With your science background and lab experience, careers like research scientist, biotechnologist, or pharmaceutical research are good options.\n",
    "\n",
    "Q: I have experience in frontend and backend development, cloud technologies, and DevOps practices. I also know JavaScript, Python, and AWS. I have a degree in Computer Science. What career paths are suitable?\n",
    "A: Since you have strong programming, cloud, and DevOps skills, I’d recommend careers such as full-stack developer, DevOps engineer, or software engineer.\n",
    "\n",
    "Q: I have Python and ML skills. What careers suit me?\n",
    "A: With your Python and ML skills, I’d suggest careers like data scientist, machine learning engineer, or AI researcher.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "user_query = \"I have skills in Analytical reasoning, Software Development, Python, Machine Learning.I have a Bachelors in Computer Engineering. What career paths should I consider?\"\n",
    "\n",
    "# Build the final prompt\n",
    "input_text = cot_examples.strip() + f\"\\n\\nQ: {user_query}\\nA:\"\n",
    "\n",
    "# Tokenize input\n",
    "batch = tokenizer(input_text, return_tensors=\"pt\")\n",
    "batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "# Generate with mixed precision\n",
    "with torch.amp.autocast(device_type=device.type):\n",
    "    output = modelLoRA.generate(\n",
    "        **batch,\n",
    "        max_new_tokens=90,\n",
    "        do_sample=False,       # turn on sampling\n",
    "        temperature=0.7,               # moderate randomness\n",
    "        top_p=0.9,                     # nucleus sampling\n",
    "        top_k=20,                      # limit candidate tokens\n",
    "        repetition_penalty=1.3,\n",
    "        eos_token_id=tokenizer.eos_token_id,  # stop when EOS is reached\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        output_scores=False\n",
    "    )\n",
    "\n",
    "# Decode only the new tokens\n",
    "generated_tokens = output[0][batch[\"input_ids\"].shape[1]:]\n",
    "output_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "print(\"Model Response:\", output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4014279f2424ac39be8a321ab228a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "\n",
    "    )\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "HUGGING_FACE_TOKEN = \"------\" # Put in your Hugging Face Token here\n",
    "\n",
    "qloramodel = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=HUGGING_FACE_TOKEN,\n",
    "    device_map='auto',\n",
    "    quantization_config=bnb_config\n",
    "    #low_cpu_mem_usage=True      # efficient loading\n",
    ")\n",
    "\n",
    "# Set token\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=HUGGING_FACE_TOKEN)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "qloraconfig = LoraConfig(\n",
    "    r = 16, # optimized\n",
    "    lora_alpha = 64, # optimized\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], # This specifies which layers (or submodules) in the model will be adapted using LoRA, key projection and value projection\n",
    "    lora_dropout = 0.05, # regularization technique that helps prevent overfitting\n",
    "    bias = \"none\", #how to handle the bias terms in the model during fine-tuning, so in case, no bias terms are updated or fine-tuned during the LoRA process\n",
    "    task_type = \"CAUSAL_LM\" #  specifies the type of task for which the model is being fine-tuned\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "qloraconfig = LoraConfig(\n",
    "    r = 16, # optimized\n",
    "    lora_alpha = 64, # optimized\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], \n",
    "    lora_dropout = 0.05, \n",
    "    bias = \"none\", \n",
    "    task_type = \"CAUSAL_LM\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='58' max='58' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [58/58 04:28, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=58, training_loss=1.4425745996935615, metrics={'train_runtime': 273.3137, 'train_samples_per_second': 13.274, 'train_steps_per_second': 0.212, 'total_flos': 1.5758903167942656e+16, 'train_loss': 1.4425745996935615, 'epoch': 2.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "qlora_model = get_peft_model(qloramodel, qloraconfig)\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model = qlora_model,\n",
    "    train_dataset = tokenized_dataset['train'],\n",
    "    args = transformers.TrainingArguments(\n",
    "        per_device_eval_batch_size = 1,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        warmup_steps = 10,\n",
    "        learning_rate = 1.71e-4, # optimized\n",
    "        fp16 = True,\n",
    "        num_train_epochs=2, # from optimized hyperparameter\n",
    "        logging_steps = 100,\n",
    "        output_dir = 'outputs',\n",
    "        report_to = \"none\"\n",
    "    ),\n",
    "    data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm = False)\n",
    ")\n",
    "qloramodel.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c593f4759b64456ca185282ba33b676a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FYP/shar0097/.conda/envs/py311/lib/python3.11/site-packages/transformers/utils/hub.py:917: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783394f09f6b461198c131380d3d9c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d938b5b2194be6a36d88fe43f3af02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/sharshar20/career-advisory-qlora-llama3.2-3b-instruct-v7/commit/0a6673f53b0b41fc0f0d4b713ef40268bb69c38b', commit_message='QLora Training method for Instruct Model (data changed), r=16, alpha=64, lr=1.71e-04', commit_description='', oid='0a6673f53b0b41fc0f0d4b713ef40268bb69c38b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/sharshar20/career-advisory-qlora-llama3.2-3b-instruct-v7', endpoint='https://huggingface.co', repo_type='model', repo_id='sharshar20/career-advisory-qlora-llama3.2-3b-instruct-v7'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sharshar20/career-advisory-qlora-llama3.2-3b-v3\n",
    "\n",
    "qlora_model.push_to_hub(\"sharshar20/career-advisory-qlora-llama3.2-3b-instruct-v7\",\n",
    "                      use_auth_token=True,\n",
    "                      commit_message = \"QLora Training method for Instruct Model (data changed), r=16, alpha=64, lr=1.71e-04\",\n",
    "                      private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2425a84fcd5442eb832e84cf67dd562a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/895 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915f0c4b7ba34ecc84bbcecceaa1c90d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3778bfe1f5244c597a66a7d0623ed5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/36.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 10 : Inferencing with trained QLoRA adapter\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "peft_model_id = \"sharshar20/career-advisory-qlora-llama3.2-3b-instruct-v7\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "modelqlora = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\n",
    "                                            return_dict = True,\n",
    "                                            quantization_config=bnb_config,\n",
    "                                            device_map = 'auto',\n",
    "                                            trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "model_qlora = PeftModel.from_pretrained(modelqlora,peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"You are a top-rated NTU career advisor chatbot.\n",
    "Be polite, concise, and helpful in providing career guidance responses.\"\"\"\n",
    "\n",
    "input_text = \"I have skills in Analytical reasoning, Software Development, Python, Machine Learning.I have a Bachelors in Computer Engineering. What career paths should I consider? \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "You should consider careers like Data Engineer, Business Analyst, Solutions Architect, DevOps Engineer, Cloud Engineer, Technical Support Engineer, Data Scientist, Operations Manager, Technical Support Specialist, Business Analyst, Data Analyst, Cloud Engineer, Solutions Architect, DevOps Engineer, Technical Support Engineer, Data Scientist, Operations Manager, Technical Support Specialist, Business Analyst, Data Analyst, Cloud Engineer, Solutions Architect, DevOps Engineer, Technical Support Engineer, Data Scientist, Operations Manager, Technical Support Specialist, Business Analyst, Data Analyst, Cloud Engineer, Solutions Architect, DevOps Engineer, Technical Support Engineer, Data Scientist, Operations Manager, Technical Support Specialist, Business Analyst, Data Analyst, Cloud Engineer, Solutions Architect, DevOps Engineer, Technical Support Engineer, Data Scientist, Operations Manager\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and move the model to the appropriate device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_qlora.to(device)  # Move the model to the chosen device\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": instruction},\n",
    "    {\"role\": \"user\", \"content\": input_text}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "outputs = model_qlora.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(text.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The possible career paths for Analytical reasoning, Software Development, Python, Machine Learning and Bachelors in Computer Engineering are: Senior Software Engineer with an experience requirement of At least 5 years. The related skills are Neural Networks, Machine Learning and Image recognition. The possible career paths are Data Scientist. The experience requirement for Data Scientist is At least 4 years. The related skills are Data Analysis, Business Analysis, SQL, NoSQL, Tableau, Power BI and Python. The possible career paths are Full Stack Developer (Python,React js) with an experience requirement of At least 3 years. The related skills are Application Development, System Analysis, Requirement Gathering, Software Design, Development, Integration, Test, Deployment, Support, Documentation, Research\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and move the model to the appropriate device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_qlora.to(device)  # Move the model to the chosen device\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": instruction},\n",
    "    {\"role\": \"user\", \"content\": input_text}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "outputs = modelLoRA.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(text.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 10: BERT F1 = 0.9198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 20: BERT F1 = 0.9234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 30: BERT F1 = 0.9206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 40: BERT F1 = 0.9208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 50: BERT F1 = 0.9134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 57: BERT F1 = 0.9101\n",
      "{'bert_precision': 0.8716595768928528, 'bert_recall': 0.9547721147537231, 'bert_f1': 0.9100630879402161}\n"
     ]
    }
   ],
   "source": [
    "# Finding BERTScore for QLoRA\n",
    "metrics = evaluate_bertscore(model_qlora, tokenizer, tokenized_dataset[\"test\"])\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b94034f22e4fc1bf8faaeb68644a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 10: BERT F1 = 0.5601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 20: BERT F1 = 0.5594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 30: BERT F1 = 0.5594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 40: BERT F1 = 0.5598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 50: BERT F1 = 0.5533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch 57: BERT F1 = 0.5502\n",
      "{'bert_precision': 0.5276824235916138, 'bert_recall': 0.5773471593856812, 'bert_f1': 0.5502427816390991}\n"
     ]
    }
   ],
   "source": [
    "## Evaluate BERTScore for the Llama 3.2 3B Instruct model - baseline comparison:\n",
    "HUGGING_FACE_TOKEN = \"-----\" # Put in your Hugging Face Token here\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=HUGGING_FACE_TOKEN,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=HUGGING_FACE_TOKEN)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "metrics = evaluate_bertscore(model, tokenizer, tokenized_dataset[\"test\"])\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
