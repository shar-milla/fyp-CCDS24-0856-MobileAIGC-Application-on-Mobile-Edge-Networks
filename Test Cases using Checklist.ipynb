{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "028539ce-55c8-446e-aec6-9ca40c3edaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: llama-cpp-python in ./.local/lib/python3.12/site-packages (0.3.16)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.local/lib/python3.12/site-packages (from llama-cpp-python) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /tc1apps/anaconda3/lib/python3.12/site-packages (from llama-cpp-python) (1.26.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in ./.local/lib/python3.12/site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /tc1apps/anaconda3/lib/python3.12/site-packages (from llama-cpp-python) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /tc1apps/anaconda3/lib/python3.12/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64da6c85-83cf-4070-9fe8-3f2462667127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the tokenized dataset from the specified directory\n",
    "test_df = pd.read_csv(\"./checklist_career_chatbot_test_cases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfe5a715-4060-459d-9b87-bcbc5789b3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3fcbfc42fd3463c92ad314a244262c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ft-q4_k_m-lora.gguf:   0%|          | 0.00/2.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (1024) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Q4_K_M\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"sharshar20/llama3.2_3B_instruct-GGUF-v7\",\n",
    "    filename=\"ft-q4_k_m-lora.gguf\"\n",
    ")\n",
    "\n",
    "llm_q4km_lora = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=1024,        # context window\n",
    "    n_threads=2,       # adjust for your CPU\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3215842-89eb-4da0-88f3-8a3517c49ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [32:54<00:00, 28.21s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[1;32m     42\u001b[0m test_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcareer_chatbot_test_results_q4km_lora_v7.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 43\u001b[0m files\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcareer_chatbot_test_results_q4km_lora_v7.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "instruction = \"\"\"You are a top-rated NTU career advisor chatbot.\n",
    "Be polite, concise, and helpful in providing career guidance responses.\"\"\"\n",
    "\n",
    "# Add output columns\n",
    "test_df[\"model_output\"] = \"\"\n",
    "\n",
    "# Apply same chat template function as training\n",
    "def apply_chat_template(user_input):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "    row_json = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "    # Generate the same formatted prompt as used during fine-tuning\n",
    "    prompt = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return prompt\n",
    "\n",
    "# Generate responses using the same chat format\n",
    "for i, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    prompt = apply_chat_template(row[\"input\"]) \n",
    "\n",
    "    try:\n",
    "        output = llm_q4km_lora(\n",
    "            prompt,\n",
    "            max_tokens=128,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            stop=[\"<|eot_id|>\", \"</s>\"]\n",
    "        )\n",
    "        response = output[\"choices\"][0][\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error at row {i}: {e}\")\n",
    "        response = \"\"\n",
    "\n",
    "    test_df.at[i, \"model_output\"] = response\n",
    "\n",
    "# Save results\n",
    "test_df.to_csv(\"career_chatbot_test_results_q4km_lora_v7.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66e835a7-4644-4c75-96e8-dbd26e8a7bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398c61a1e88f40539f930f4e4f07f44d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ft-q5_k_m-lora.gguf:   0%|          | 0.00/2.32G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (1024) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Q5_K_M\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"sharshar20/llama3.2_3B_instruct-GGUF-v7\",\n",
    "    filename=\"ft-q5_k_m-lora.gguf\"\n",
    ")\n",
    "\n",
    "llm_q5km_lora = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=1024,        # context window\n",
    "    n_threads=2,       # adjust for your CPU\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b43a9d38-98ec-456b-bc9e-4293c1319769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [56:18<00:00, 48.27s/it] \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "instruction = \"\"\"You are a top-rated NTU career advisor chatbot.\n",
    "Be polite, concise, and helpful in providing career guidance responses.\"\"\"\n",
    "\n",
    "# Add output columns\n",
    "test_df[\"model_output\"] = \"\"\n",
    "\n",
    "# Apply same chat template function as training\n",
    "def apply_chat_template(user_input):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "    row_json = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "    # Generate the same formatted prompt as used during fine-tuning\n",
    "    prompt = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return prompt\n",
    "\n",
    "# Generate responses using the same chat format\n",
    "for i, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    prompt = apply_chat_template(row[\"input\"]) \n",
    "\n",
    "    try:\n",
    "        output = llm_q5km_lora(\n",
    "            prompt,\n",
    "            max_tokens=128,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            stop=[\"<|eot_id|>\", \"</s>\"]\n",
    "        )\n",
    "        response = output[\"choices\"][0][\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error at row {i}: {e}\")\n",
    "        response = \"\"\n",
    "\n",
    "    test_df.at[i, \"model_output\"] = response\n",
    "\n",
    "# Save results\n",
    "test_df.to_csv(\"career_chatbot_test_results_q5km_lora_v7.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39702e54-fb18-44cc-a2a9-39b50a3f0d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8432363f70564a9eaa64b312315246b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ft-q8_0-lora.gguf:   0%|          | 0.00/3.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (1024) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Q8_0\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"sharshar20/llama3.2_3B_instruct-GGUF-v7\",\n",
    "    filename=\"ft-q8_0-lora.gguf\"\n",
    ")\n",
    "\n",
    "llm_q8_0_lora = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=1024,        # context window\n",
    "    n_threads=2,       # adjust for your CPU\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79ee4f25-2845-4bd6-b5bc-c69b70c7adb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [44:27<00:00, 38.11s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "instruction = \"\"\"You are a top-rated NTU career advisor chatbot.\n",
    "Be polite, concise, and helpful in providing career guidance responses.\"\"\"\n",
    "\n",
    "# Add output columns\n",
    "test_df[\"model_output\"] = \"\"\n",
    "\n",
    "# Apply same chat template function as training\n",
    "def apply_chat_template(user_input):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "    row_json = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "    # Generate the same formatted prompt as used during fine-tuning\n",
    "    prompt = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return prompt\n",
    "\n",
    "# Generate responses using the same chat format\n",
    "for i, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    prompt = apply_chat_template(row[\"input\"]) \n",
    "\n",
    "    try:\n",
    "        output = llm_q8_0_lora(\n",
    "            prompt,\n",
    "            max_tokens=128,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            stop=[\"<|eot_id|>\", \"</s>\"]\n",
    "        )\n",
    "        response = output[\"choices\"][0][\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error at row {i}: {e}\")\n",
    "        response = \"\"\n",
    "\n",
    "    test_df.at[i, \"model_output\"] = response\n",
    "\n",
    "# Save results\n",
    "test_df.to_csv(\"career_chatbot_test_results_q8_0_lora_v7.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044d8934-9437-4c04-aabd-916cdd8c9412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
